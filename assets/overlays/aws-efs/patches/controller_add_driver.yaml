kind: Deployment
apiVersion: apps/v1
metadata:
  name: aws-efs-csi-driver-controller
  namespace: ${NAMESPACE}
  annotations:
    config.openshift.io/inject-proxy: csi-driver
    config.openshift.io/inject-proxy-cabundle: csi-driver
spec:
  selector:
    matchLabels:
      app: aws-efs-csi-driver-controller
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  template:
    metadata:
      labels:
        app: aws-efs-csi-driver-controller
      annotations:
        openshift.io/required-scc: privileged
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
    spec:
      hostNetwork: true
      serviceAccount: aws-efs-csi-driver-controller-sa
      priorityClassName: system-cluster-critical
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: "NoSchedule"
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: "NoSchedule"
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: aws-efs-csi-driver-controller
                topologyKey: kubernetes.io/hostname
      initContainers:
        - name: init-aws-credentials-file
          image: ${TOOLS_IMAGE}
          command:
          - sh
          - -c
          - |
            # Define file path variables
            CREDENTIALS_FILE=/var/run/aws/keys/credentials
            AUTH_CREDENTIALS_FILE=/var/run/aws/auth/credentials
            AWS_ACCESS_KEY_ID_FILE=/var/run/aws/keys/aws_access_key_id
            AWS_SECRET_ACCESS_KEY_FILE=/var/run/aws/keys/aws_secret_access_key

            # If credentials key exists in ebs-cloud-credentials secret, then use it as the auth file
            if [ -e "$CREDENTIALS_FILE" ]; then
                cp "$CREDENTIALS_FILE" "$AUTH_CREDENTIALS_FILE"
                echo "Kubernetes Secret already contains credentials file, copied to the right place: $AUTH_CREDENTIALS_FILE"
                exit 0
            fi

            # Otherwise, make sure the access keys are mounted in the pod...
            if [ ! -e "$AWS_ACCESS_KEY_ID_FILE" ] || [ ! -e "$AWS_SECRET_ACCESS_KEY_FILE" ]; then
                echo "AWS keys not found"
                exit 1
            fi

            # And create an auth file based on those keys
            cat <<-EOF > "$AUTH_CREDENTIALS_FILE"
            [default]
            aws_access_key_id=$(cat "$AWS_ACCESS_KEY_ID_FILE")
            aws_secret_access_key=$(cat "$AWS_SECRET_ACCESS_KEY_FILE")
            EOF
            echo "Kubernetes Secret does not have credentials file, created a fresh one at $AUTH_CREDENTIALS_FILE"
          volumeMounts:
            - name: aws-keys
              mountPath: /var/run/aws/keys
              readOnly: true
            - name: aws-auth
              mountPath: /var/run/aws/auth
          terminationMessagePolicy: FallbackToLogsOnError
      containers:
        # CSI driver container
        - name: csi-driver
          image: ${DRIVER_IMAGE}
          imagePullPolicy: IfNotPresent
          args:
            - --endpoint=$(CSI_ENDPOINT)
            - --logtostderr
            - --tags=kubernetes.io/cluster/${CLUSTER_ID}:owned
            - --delete-access-point-root-dir=true
            - --v=${LOG_LEVEL}
          env:
            - name: CSI_ENDPOINT
              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
            - name: AWS_SDK_LOAD_CONFIG
              value: '1'
            - name: AWS_CONFIG_FILE
              value: /var/run/aws/auth/credentials
          ports:
            - name: healthz
              # Due to hostNetwork, this port is open on a node!
              containerPort: 10302
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 10
            timeoutSeconds: 3
            periodSeconds: 10
            failureThreshold: 5
          securityContext:
            # The driver needs to be privileged to be able to mount + clean EFS sub-directories that are used as PVs
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - name: aws-auth
              mountPath: /var/run/aws/auth
              readOnly: true
            - name: bound-sa-token
              mountPath: /var/run/secrets/openshift/serviceaccount
              readOnly: true
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
          resources:
            requests:
              memory: 50Mi
              cpu: 10m
            # The CSI driver can consume a lot of memory if many volumes are created at once. This is
            # intended to prevent the driver from adding undue stress to control-plane nodes.
            limits:
              memory: 1Gi
          # external-provisioner container
        - name: csi-provisioner
          image: ${PROVISIONER_IMAGE}
          imagePullPolicy: IfNotPresent
          args:
            - --csi-address=$(ADDRESS)
            - --feature-gates=Topology=true
            - --extra-create-metadata=true
            - --http-endpoint=localhost:8212
            - --leader-election
            - --leader-election-lease-duration=${LEADER_ELECTION_LEASE_DURATION}
            - --leader-election-renew-deadline=${LEADER_ELECTION_RENEW_DEADLINE}
            - --leader-election-retry-period=${LEADER_ELECTION_RETRY_PERIOD}
            - --v=${LOG_LEVEL}
            - --timeout=5m
            - --worker-threads=1
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          securityContext:
            # The container needs to be privileged to be able to talk to the driver CSI socket, which was created by a privileged container
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
          resources:
            requests:
              memory: 50Mi
              cpu: 10m
          # kube-rbac-proxy for external-provisioner container.
          # Provides https proxy for http-based external-provisioner metrics.
        - name: provisioner-kube-rbac-proxy
          args:
            - --secure-listen-address=0.0.0.0:9212
            - --upstream=http://127.0.0.1:8212/
            - --tls-cert-file=/etc/tls/private/tls.crt
            - --tls-private-key-file=/etc/tls/private/tls.key
            - --tls-cipher-suites=${TLS_CIPHER_SUITES}
            - --tls-min-version=${TLS_MIN_VERSION}
            - --logtostderr=true
          image: ${KUBE_RBAC_PROXY_IMAGE}
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9212
              name: provisioner-m
              protocol: TCP
          resources:
            requests:
              memory: 20Mi
              cpu: 10m
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - mountPath: /etc/tls/private
              name: metrics-serving-cert
        - name: csi-liveness-probe
          image: ${LIVENESS_PROBE_IMAGE}
          imagePullPolicy: IfNotPresent
          args:
            - --csi-address=/csi/csi.sock
            - --probe-timeout=3s
            - --health-port=10302
            - --v=${LOG_LEVEL}
          securityContext:
            # The container needs to be privileged to be able to talk to the driver CSI socket, which was created by a privileged container
            privileged: true
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
          resources:
            requests:
              memory: 50Mi
              cpu: 10m
      volumes:
        - name: aws-keys
          secret:
            secretName: aws-efs-cloud-credentials
        - name: aws-auth
          emptyDir: {}
        # This service account token can be used to provide identity outside the cluster.
        # For example, this token can be used with AssumeRoleWithWebIdentity to authenticate with AWS using IAM OIDC provider and STS.
        - name: bound-sa-token
          projected:
            sources:
              - serviceAccountToken:
                  path: token
                  audience: openshift
        - name: socket-dir
          emptyDir: {}
        - name: metrics-serving-cert
          secret:
            secretName: aws-efs-csi-driver-controller-metrics-serving-cert
